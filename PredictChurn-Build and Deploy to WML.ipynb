{"cells": [{"cell_type": "markdown", "source": "## Predict Customer Churn Use Case Implementation\nThe objective is to follow the CRISP-DM methodology to build a model to predict customer churn, and operationalize the model by deploying it into WML\n![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n\n### Step 1: Download the customer churn data", "metadata": {}}, {"cell_type": "code", "source": "#Run once to install the wget package\n!pip install wget", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# download data from GitHub repository\nimport wget\nurl_churn='https://raw.githubusercontent.com/SidneyPhoon/IntroToDSX-WML/master/data/churn.csv'\nurl_customer='https://raw.githubusercontent.com/SidneyPhoon/IntroToDSX-WML/master/data/customer.csv'\n\n#remove existing files before downloading\n!rm -f churn.csv\n!rm -f customer.csv\n\nchurnFilename=wget.download(url_churn)\ncustomerFilename=wget.download(url_customer)\n\n#list existing files\n!ls -l churn.csv\n!ls -l customer.csv", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 2: Read data into Spark DataFrames\n\nNote: You want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame", "metadata": {}}, {"cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nchurn= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"churn.csv\")\n\ncustomer = spark.read\\\n    .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(\"customer.csv\")", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 3: Merge Files\n", "metadata": {}}, {"cell_type": "code", "source": "data=customer.join(churn,customer['ID']==churn['ID']).select(customer['*'],churn['CHURN'])\ndata.toPandas().head()", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "scrolled": false}}, {"cell_type": "markdown", "source": "### Step 4: Rename some columns\nThis step is not a requirement, it just makes some columns names simpler to type with no spaces", "metadata": {}}, {"cell_type": "code", "source": "# withColumnRenamed renames an existing column in a SparkDataFrame and returns a new SparkDataFrame\n\ndata = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.toPandas().head()", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 5: Data understanding", "metadata": {}}, {"cell_type": "markdown", "source": "### Dataset Overview", "metadata": {}}, {"cell_type": "code", "source": "df_pandas = data.toPandas()\nprint \"There are \" + str(len(df_pandas)) + \" observations in the customer history dataset.\"\nprint \"There are \" + str(len(df_pandas.columns)) + \" variables in the dataset.\"\n\nprint \"\\n******************Descriptive statistics*****************************\\n\"\nprint df_pandas.drop(['ID'], axis = 1).describe()\n", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Exploratory Data Analysis", "metadata": {}}, {"cell_type": "markdown", "source": "The **Brunel** Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and more aggressive business users. The system interprets the language and produces visualizations using the user's choice of existing lower-level visualization technologies typically used by application engineers such as RAVE or D3. \n\nMore information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here:  http://brunel.mybluemix.net/gallery_app/renderer", "metadata": {}}, {"cell_type": "code", "source": "import brunel\ndf_pandas = data.toPandas()\n%brunel data('df_pandas') stack bar x(Paymethod) y(#count) color(CHURN) bin(Paymethod) percent(#count) label(#count) tooltip(#all) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 ", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# Heat map\n%brunel data('df_pandas') x(LocalBilltype) y(Dropped) color(#count:red) style('symbol:rect; size:100%; stroke:none') tooltip(Dropped,#count)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.<br/>\nMore information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969", "metadata": {}}, {"cell_type": "markdown", "source": "**If you haven't already installed it, uncomment and run the following cell to install the pixiedust Python library in your notebook environment. You only need to run it once**\n", "metadata": {}}, {"cell_type": "code", "source": "#!pip install --user --upgrade pixiedust", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "scrolled": true}}, {"cell_type": "code", "source": "from pixiedust.display import *\ndisplay(data)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "pixiedust": {"displayParams": {"chartsize": "51", "keyFields": "Paymethod", "aggregation": "AVG", "handlerId": "barChart", "valueFields": "Usage", "rowCount": "500"}}}}, {"cell_type": "markdown", "source": "### Interactive query with Spark SQL", "metadata": {}}, {"cell_type": "code", "source": "# Spark SQL also allow you to use standard SQL\ndata.createOrReplaceTempView(\"data\")\nsql = \"\"\"\nSELECT c.*\nFROM data c\nWHERE c.EstIncome>90000\n\n\"\"\"\nspark.sql(sql).toPandas().head()", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 6: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models. A pipeline defines a sequence of transformers and estimators to perform tha analysis in stages.<br/>\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html", "metadata": {}}, {"cell_type": "code", "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# StringIndexer encodes a string column of labels to a column of label indices. \nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# encode the label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# build the pipeline\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# Split data into train and test datasets\n(trainingData, testingData) = data.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(trainingData)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 7: Score the test data set", "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "result=model.transform(testingData)\nresult_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\nresult_display.toPandas().head(6)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 8: Model Evaluation\nFind accuracy of the models and the Area Under the ROC Curve ", "metadata": {}}, {"cell_type": "code", "source": "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "###  Step 9:  Tune the model to find the best model", "metadata": {}}, {"cell_type": "markdown", "source": "#### Build a Parameter Grid specifying the parameters to be evaluated to determine the best combination", "metadata": {}}, {"cell_type": "code", "source": "# set different levels for the maxDepth\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "#### Create a cross validator to tune the pipeline with the generated parameter grid\nCross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one.", "metadata": {}}, {"cell_type": "code", "source": "# perform 3 fold cross validation\ncv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# train the model\ncvModel = cv.fit(trainingData)\n\n# pick the best model\nbest_rfModel = cvModel.bestModel", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# score the test data set with the best model\ncvresult=best_rfModel.transform(testingData)\ncvresults_show=cvresult.select(cvresult[\"ID\"],cvresult[\"CHURN\"],cvresult[\"Label\"],cvresult[\"predictedLabel\"],cvresult[\"prediction\"],cvresult[\"probability\"])\ncvresults_show.toPandas().head()", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "\nprint 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\nprint 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\nprint '   '\nprint('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\nprint 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 10: Save Model in WML repository\n\nIn this section you will store your model in the Watson Machine Learning (WML) repository by using Python client libraries.\n* <a href=\"https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html\">WML Documentation</a>\n* <a href=\"http://watson-ml-api.mybluemix.net/\">WML REST API</a> \n* <a href=\"https://watson-ml-staging-libs.mybluemix.net/repository-python/\">WML Repository API</a>\n<br/>\n\nFirst, you must import client libraries.", "metadata": {}}, {"cell_type": "code", "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "If you do not already have an instance of Watson Machine Learning service in Bluemix, go to <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a>, click **Catalog** on the top right of the menu, search for \"Machine Learning\", and create an instance.\n\nPut your authentication information from your instance of the Watson Machine Learning service in <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a>, into the next cell. You can find your information in the **Service Credentials** tab of your service instance in Bluemix.\n\n![WML Credentials](https://raw.githubusercontent.com/SidneyPhoon/IntroToWMLLab/master/images/WML%20Credentials.png)\n\n<span style=\"color:red\">Replace the service_path and credentials with your own information</span>\n\nservice_path=[your url]<br/>\ninstance_id=[your instance_id]<br/>\nusername=[your username]<br/>\npassword=[your password]<br/>", "metadata": {}}, {"cell_type": "code", "source": "# @hidden_cell\nservice_path = 'https://ibm-watson-ml.mybluemix.net'\ninstance_id = 'XXXXXX'\nusername = 'XXXXX'\npassword = 'XXXXXX'", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "Authorize the repository client:", "metadata": {}}, {"cell_type": "code", "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "Create the model artifact.\n\n<b>Tip:</b> The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).", "metadata": {}}, {"cell_type": "code", "source": "model_artifact = MLRepositoryArtifact(model, training_data=trainingData, name=\"Predict Customer Churn\")", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "Save model artifact to your Watson Machine Learning instance:", "metadata": {}}, {"cell_type": "code", "source": "saved_model = ml_repository_client.models.save(model_artifact)", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": "# Print the saved model properties\nprint \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 11: Generate the Authorization Token for Invoking the model", "metadata": {}}, {"cell_type": "code", "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v2/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "### Step 12:  Go to WML in Bluemix to create a Deployment Endpoint\n\n* In your <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> dashboard, click into your WML Service and click the **Launch Dashboard** button under Watson Machine Learing.\n![WML Launch Dashboard](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/WML_Launch_Dashboard.png)\n\n<br/>\n* You should see your deployed model in the **Models** tab", "metadata": {}}, {"cell_type": "markdown", "source": "* Under *Actions*, click on the 3 ellipses and click ***Create Deployment***.  Give your deployment configuration a unique name, e.g. \"Predict Customer Churn Deply\", select Type=Online and click **Save**.\n<br/>\n<br/>\n* In the *Deployments tab*, under *Actions*, click **View Details**\n<br/>\n<br/>\n* Scoll down to **API Details**, copy the value of the **Scoring Endpoint** into your notepad.  (e.g. \thttps://ibm-watson-ml.mybluemix.net/v2/published_models/64fd0462-3f8a-4b42-820b-59a4da9b7dc6/deployments/7d9995ed-7daf-4cfd-b40f-37cb8ab3d88f/online)", "metadata": {}}, {"cell_type": "markdown", "source": "### Step 13:  Invoke the model through REST API call", "metadata": {}}, {"cell_type": "markdown", "source": "#### Create a JSON Sample record for the model ", "metadata": {}}, {"cell_type": "code", "source": "json_payload = {\n    \"fields\": [\n    \"ID\",\n    \"Gender\",\n    \"Status\",\n    \"Children\",\n    \"EstIncome\",\n    \"CarOwner\",\n    \"Age\",\n    \"LongDistance\",\n    \"International\",\n    \"Local\",\n    \"Dropped\",\n    \"Paymethod\",\n    \"LocalBilltype\",\n    \"LongDistanceBilltype\",\n    \"Usage\",\n    \"RatePlan\"\n    ],\n    \"values\": [ [999,\"F\",\"M\",2.0,77551.100000,\"Y\",33.600000,20.530000,0.000000,41.890000,1.000000,\"CC\",\"Budget\",\"Standard\",62.420000,2.000000] ]\n} \n", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "#### Make Rest API call to test the deployed model", "metadata": {}}, {"cell_type": "code", "source": "# Get the scoring endpoint from the WML service\n# Replace the value for scoring_endpoint with your own scoring endpoint\nscoring_endpoint = 'XXXXXXX'\nheader_online = {'Content-Type': 'application/json', 'Authorization': \"Bearer \" + mltoken}\n\n# API call here\nresponse_scoring = requests.post(scoring_endpoint, json=json_payload, headers=header_online)\n\nprint response_scoring.text", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "#### Grab Predicted Value ", "metadata": {}}, {"cell_type": "code", "source": "wml = json.loads(response_scoring.text)\n\n# First zip the fields and values together\nzipped_wml = zip(wml['fields'], wml['values'].pop())\n\n# Next iterate through items and grab the prediction value\nprint(\"Predicted Churn: \" + [v for (k,v) in zipped_wml if k == 'predictedLabel'].pop())", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "You have come to the end of this notebook", "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": "**Sidney Phoon**<br/>\nSeptember 5th, 2017", "metadata": {}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}, "language_info": {"pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}, "version": "2.7.11", "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python"}}}